# 第三章 基于特征解释的难点和主观性

本论文的主要目的之一是验证解释是否忠实地描述了它们所要解释的模型的决策过程。首先，我的重点是介绍一个框架来验证基于特征的事后解释方法所提供解释的真实性。要做到这一点，第一个挑战是定义什么是忠实的基于特性的解释。在本章的3.1节中，我展示了对于某些模型和实例，存在不止一个基于真实值特征的解释。因此，解释(或解释器)的忠实程度取决于实践中所偏好的基础真实解释的类型。

​		此外，我指出两种有影响力的解释器，即Shapley解释器和最小充分子集解释器，针对不同类型的真实值解释。我还证明，在某些情况下，它们都不足以提供决策过程的完整视图。据我所知，文献中并没有强调可能有不止一种基于真实值特征的解释，也没有强调每个解释器都坚持哪一种基础真实值解释。然而,这些至关重要的信息对于用户,选择最适合他们的需求和解释正确的解释器，对于研究人员告诉用户预期行为、优点，和解释器的局限性,以及提供公平的自动评估解释方法比较。相反，有影响力的著作似乎暗示，只有一种基于真实值特征的解释是所有解释器的目标。

​		在验证解释的真实性时，第二个挑战的方面恰恰是这样一个事实:一般来说，我们不知道神经模型的决策过程。为了解决这个问题，在本章的3.2节中，我研究了期望提供忠实解释的基于特性的自解释模型在多大程度上确实做到了这一点。这类模型将在第4章中作为验证基于特征的解释方法的实验平台。

​		本章的发现部分基于(Camburu et al.， 2019)，不仅是我将在第4章介绍的验证框架的重要基础，而且就其本身而言，因为它们指向了可解释性的基本方面。

​		**说明例子**  在深入本章的核心内容之前，我向读者介绍了我将在以后广泛使用的举例说明的类型。更确切地说，我将使用假设模型来进行文本情绪分析，以举例说明本章的观点。这样的模型将评论作为输入，例如，一段包含对一个对象的意见的文本，并输出一个分数，这个分数反映了评论对该对象或对象的一个方面的看法。该任务是自然语言处理中的一个突出任务，具有重要的现实世界应用(McAuley et al.， 2012;Maas等，2011)。与Lei et al.(2016)类似，我将任务视为回归，得分为一个实数，线性反映情绪的强度。在这些例子中，-1是最负的分数，1是最正的分数，因此0代表中立的分数。我们假设差异至少为0.1才显著。情绪分析任务的一种重要类型是在多方面回顾中识别对某一方面的情绪。

![](..\image\3.1.png)

图3.1:Shapley解释器充分子集解释器分别给出至少两个基于真实特征的解释的例子。Shapley值由式2.3计算。假设分数是线性反映情绪的强度，0.1的差异是显著的。

例如，包含对该对象的多个方面的意见的综述。例如，BeerAdvocate数据集(McAuley et al.， 2012)包含了人类撰写的评论，涵盖了啤酒的四个方面，即外观、口感、味道和气味。在这一章中，我将使用几个假设模型的例子来进行整体和多方面的情绪分析作为例证。



## 3.1 各种基于特征真实值的解释器

在本节中，我将展示一些模型和实例的案例，这些案例中存在不止一个基于真实值特征的解释。我还展示了两类流行的解释器，他们各自提倡不同的事实基础解释，但没有明确提及。据我所知，迄今为止的文献中还没有强调这一点。相反，目前有影响力的著作似乎暗示，对于一个模型的预测只有一种基于真实值特征的解释。尽管如此，用户和研究人员都必须意识到这些差异。

​		此后，我将把明确旨在提供Shapley值作为解释的任何解释器标记为Shapley解释器。正如我们在第2章中看到的，Lundberg 27和Lee(2017)认为所有的特征加性解释器都应该以Shapley值作为解释的目标。然而，鉴于本章的发现，我想允许特征加性解释器坚持不同的观点。

​		为了说明真实存在的多个特征对于一个实例的解释预测模型,在图3.1中,我提出一个假设的例子情绪分析回归模型。该模型使其预测如下:子字符串“very good”在输入文本的存在会导致一个非常积极的得分为0.9分。在没有“very good”的情况下，如果输入文本中出现了“nice”，那么模型提供了0.7的积极情绪。如果输入文本中既没有出现“very good”，也没有出现“nice”，那么模型就会给出0.6分，最后，如果这些积极指标都没有出现，那么模型就会默认得到中性的0分。因此，这是一个我们知道其决策过程的简单模型。将这个模型应用到实例$x_{(1)}$上:“The movie was good, it was actually nice."。，模型预测$m(x_{(1)})$ = 0.7，因为“nice”出现在输入实例中。因此，可以认为“nice”是0.7的预测中唯一重要的特性，而“good”则是另一个预测(0.6)中唯一重要的特性。然而，有人可能会辩称，对于这种预测，“good”也必须被标记为重要，原因如下:如果要取消“good”，那么该模型将依赖“good”给出高达0.6的分数，而不是低得多的0分。因此，我们看到对于基于特征真实值的解释应该是什么，即使对于这个简单的模型，也有两种同样有效的观点。因此，在实践中选择哪种视角是主观的。

​		当试图用基于特征的解释来解释模型$m$时所面临的困难在实例$x_{(2)}$上更加明显:“The movie was nice, in fact, it was very good.”模型预测$m(x_{(2)}) = 0.9$，因为“very good”是输入实例的子字符串。因此，特征“very good”和“good”构成了对这个预测的一个基本事实解释。然而，在这个例子中，如果“good”被剔除，模型依赖于“nice”(而不是“very”)来提供高达0.7的分数，而如果“good”和“nice”都被剔除，那么分数就会一直下降到0。因此，从这个角度来看，“nice”可以被视为比“very”更重要，按照重要性顺序提供“good”、“nice”、“very”的解释也是基于真实值特征的解释。

​		**Shapley值与最小充分子集**。上面描述的两种基本真实解释是由Shapley值和最小充分子集解释器这两类有影响力的解释器分别倡导的。一方面，Shapley 解释器 (Lundberg and Lee, 2017)告诉我们，在这个实例$x^{(1)}$上，“nice”是该模型最重要的特征，权重为0.4，但“good”的显著贡献为0.3(见公式2.3)。另一方面，最小充分子集解释器发现只有“nice”特性是重要的(见公式2.4和2.5)。类似地，对于模型$m$对实例$x_{(2)}$的预测，Shapley解释告诉我们，“good”和“nice”是两个最重要的特征，重要性非常接近，而“very”的重要性大约是“nice”的三倍。另一方面，最小充分子集的观点告诉我们，两个最重要的特性是“good”和“very”，而“nice”根本不会被提到重要。

​		两种类型的真实值解释之间的区别在于，与 Shapley 值一致的解释旨在为实例邻域的要素提供*平均重要性权重* ，而与最小充分子集对齐的解释旨在提供模型在实例单独上使用的点态特性。Shapley值来自合作博弈论(Shapley, 1951)，它的引入是为了促进在参与者之间分配联盟总收益的公平性。这就是Shapley值通过方程2.2中的一致性条件考虑玩家在任意子联盟中的表现能力的原因。另一方面，最小充分子集视角只奖励完全必要的联盟内的玩家。在图3.1的例子中，对于模型$m$，“nice”在“very”和“good”相邻出现的情况下是不必要的，即使“nice”单独就能得到可观的0.7分。

​		在此之后，我将把这两种真实值解释分别称为邻域解释和点态解释。这些名字是为了真实反映这两个解释的高层根本区别,对m关于$x^{(2)}$的预测,如考虑“nice”是比“very”更重要的与考虑“nice”作为解释是不重要，而不是低层次的差异,如具体的Shapley值权重或最小充分子集提供子作为解释而不是权重，我将明确地称为Shapley解释(或Shapley explainer)和最小充分子集解释(或最小充分子集解释器)，以避免混淆。

**不总是区分。** 尽管对于某些模型和实例，比如上面提到的例子，这两种真实值解释是截然不同的，但在很多情况下，它们是一致的。例如，对于相同的模型$m$，但是应用于只包含三个关键子短语“very good”、“nice”和“good”中的一个的实例，例如“The movie was good”。邻域解释和点态解释都指向同样重要的特征。

**没有在文献中强调。**据我所知，Sundararajan等人(2017)在实例$x_1= 1, x_2= 3$上解释函数$min(x1, x2)$的例子中，只是简要地提到了上述两种基于特征的真实值解释之间的根本区别。他们的方法称为综合梯度，将所有重要度权重( $1 = min(1,3)$)赋值给关键特征$x_1$(因此对$x_2$的重要性为0)，而Shapley-Shubik (Shapley and Shubik, 1971)属性为每个特征赋值为$0.5 =\frac{1}{2}min(1,3)$权重。他们还提到，偏爱一种解释而不喜欢另一种解释是一种主观问题。

​		另一方面,她和李的有影响力的工作(2017)所示,只有一个图表的结果(图4),在他们的用户研究亚马逊机械Turk,所有参与者提供的解释对于方法$max(x_1, x_2, x_3$) 在输入为$x_1 = 5, x_2 = 4, x_3 = 0$, $x1$被赋予3的重要性权重,$x_2$被赋予2的权重， $x_3$被赋予0的权重。作者在他们的用户研究中没有提到参与者的数量或特征，也没有提供他们给参与者的精确指导。有必要对用户进行更广泛和有充分文件证明的研究，以得出一个基于事实的解释比另一个更有用的结论。

​	当前工作中存在的另一种模式可能会阻碍人们承认一个以上的事实解释以及Shapley解释器和最小子集解释器之间的根本区别，因为他们直接比较了不同类别的解释器识别模型用于预测的真实特征的能力。例如,陈et al。(2018)和Yoon et al。(2019)比较L2X,一个最小充分子集解释器和shapley方法(她和李,2017)，在已知真实特征的合成数据集上训练的模型使用的真实特征，称为二维异或、Orange Skin和开关特征。虽然这些特殊的合成数据集碰巧没有违反上面描述的两种基本事实解释中的任何一种，但是在比较的时候这些信息没有被提及。这样的比较有可能导致这样一种想法，即所有基于特征的解释器都应该致力于寻找一种基于真实特征的解释。

### 3.1.1 优点和局限

本节揭示了上面介绍的两种基于特征的解释的优点和局限性。

**冗余特征**。仅看图3.1中对$m(x^1)$的Shapley解释，我们无法知道(1)模型是否需要“nice”和“good”两个特征来做出0.7的预测(m不是这样)，还是(2)其中一个属性在其他属性存在时是冗余的(这是m的情况)。相反，最小充分子集解释不包含冗余特征(因为它们会违反方程2.5)，因此，$m(x^1)$的最小充分子集解释能够区分两种情况。

![image-20201021195208340](..\image\3.2.png)

图3.2:举例说明两种基于特征的解释的优点和局限性，如3.1.1节所示。这五个规则对所有三种模型都是通用的。Shapley值由式2.3计算，并按其重要性(绝对值)递减排列;未提及的功能获得0权重。MSS表示最小充分子集。在最后一个例子中，“amazing”的上标区别了它的两次出现。



**取消特征：真实 vs 人工**。 在某些情况下，存在一些相互抵消的特征。在实例$x^O$上，通过考虑所有三个方面，可以得出结论1。然而，最小的充分子集解释是{“amazing”，“appearance”}，由于公式2.5，它不包含特征“bad”，“taste”，“good”，“smell”。有争议的是，用户可能希望在解释中看到这些被真正取消的特征。请注意，这些特征被Shapley解释标记为重要的，尽管如此，这并不能清楚地表明“good smell”和“bad taste”之间的完美消除。此外，Shapley的解释给人的印象是“smell”和“appearance”(0.15和0.12)比“taste”(0.03)重要得多，而$m^O$通过设计，平等地考虑了所有方面。因此，Shapley和最小充分子集解释都不能很好地反映$m^O(x^O)$的决策过程。

​		当从实例中消除特性时，可能会出现人工操作，从而扭曲某些特性的重要性。图3.2说明了这样一个例子。当$m^S$应用于$x^{S1}$时，预测值为−0.3，最小充分子集解释为{“peculiar”，“smell”}，可以说，这最能反映$m^S(x^{S1})$的决策过程。然而，在Shapley的解释中，“taste”似乎比“peculiar”重要两倍，“horrible”似乎和“peculiar”一样重要，尽管“peculiar”是嗅觉的实际情绪指标。此外，请注意，在实例$x^{S2}$中，即使$m^S$并不依靠对味道的感觉来预测对气味的感觉，但当只有对味道的感觉发生变化时，Shapley重要性权重戏剧性地发生变化。



**多重最小充分子集：真实vs人工**。在某些情况下，可能存在多个最小充分子集解释一个预测。例如，对于图3.2中的模型$m^T$和实例$x^{T1}$，特性“good”和“refreshing”中的任何一个都可以得到0.6分。理想情况下，最小充分子集解释器提供所有真正的最小充分子集，例如，{“taste”，“good”}和{“taste”，“refreshing”}。然而，许多极小充分子集解释器被设计为只检索一个极小充分子集(Chen et al.， 2018a;Yoon等，2019)。一个例外是SIS explainer (Carter et al.， 2019)，它检索一组不相关的最小充分子集，这也可能是不详尽的(例如，SIS不会检索$m^T(x^{T1})$的第二个最小充分子集解释，因为“taste”已经被第一个最小充分子集占有)。另一方面，shapley的解释认为“good”和“refreshing”同样重要。然而，单凭这种解释，我们无法知道两者是否都有必要出现，或者某一个单独出现是否呢巩固实现0.6的预测。

​		消除特性时发生的人工制品也可能导致某些特征子集出现为足够的最小子集。例如，对于图3.2中的$m^Ta和x^{T2}$，两个“amazing”出现中的任何一个都构成了一个足够的最小子集，但是第二个没有反映模型的决策过程。Shapley解释在这个案例中做出了区别。

​		在本节中，我们看到在某些情况下，可以有多个基于真实特征的解释来解释一个模型的预测。我确定了两种解释类型，并称之为邻域解释和点态解释。我们还看到，Shapley解释器的目的是提供邻域解释，而最小充分子集解释者的目的是提供点态解释。两种类型的解释和解释器都为目标模型的决策过程提供了有价值的见解，它们都有各自的长处和局限性。因此，在不同的实际用例中，一种类型的解释可能比另一种更可取，因此，底层真相的选择最好留给用户。因此，用户需要了解不同类型的解释和解释器之间的优势、局限性和差异，以便选择和使用它们。

​		最后，同时使用邻域和点态解释可以提供一个模型决策过程的完整画面。因此，将这两种解释结合起来的研究将是一项有趣的未来工作。此外，未来的工作可能包括一项全面的用户研究，以确定用户从每种解释以及它们的组合中获益的程度。

​		最重要的是，本部分所描述的工作鼓励研究者特别注意他们的解释方法所要提供的解释背后的假设，并在他们的工作中直接陈述它们。例如，一个好的实践可以是，每一本介绍解释器或解释器验证框架的著作都有一个章节说明，其中作者陈述了他们方法的预期行为、优势和局限性。

## 3.2 信任选择器-预测器模型

验证事后解释器的可信度的一种潜在方法是使用基于特征的自解释模型作为实验平台，因为这些模型应该提供与预测相关的特征。在本节中，我将研究一种特殊类型的基于特性的自解释模型，我称之为选择器-预测器模型。**一个选择器-预测器模型由两个顺序模块组成:一个选择器和一个预测器。**选择器对输入特征的子集进行硬选择，只有选择的特征被传递给预测器，预测器输出最终的答案。对于用于建模选择器和预测器的神经网络类型没有限制。关键的是，只要选择器和预测器与监督一起训练最终的答案，就不需要对选择的特征进行监督。因此，这种基于特性的自解释模型非常吸引人。例如，Lei等人(2016)、Yoon等人(2019)和Chang等人(2019)引入了遵循这种高级架构的模型。

​		我将研究选择器-预测器自解释模型的类型，因为对于任何实例来说，它似乎都方便地提供了一组相关的特性(被选中的)和不相关的特性(可能是未被选中的)。因此，似乎可以测试解释器是否正确地将不相关的特征识别为不相关的特征。然而，我们将在下面看到，这种类型的自解释模型可能并不总是忠实地解释自己。

​		形式上，让我们定义$D$是一个包含$x$的数据集，其中$x$有$n$个特征$x=(x_1.x_2,...,x_n)$。例如，$x$可以是一个句子，$x_i$是句子中的第$i$个token。选择器我们称之为$sel(·)$，输入$x$并返回一组特征子集$S^m_x \subseteq x$。我们将$pred(·)$称为预测器，仅仅将特征子集$S^m_x:=sel(x)$输入，并仅仅通过这组特征子集做出预测，公式如下：

{% math %}
$$
m(x)=pred(sel(x))=pred(S^m_x)
$$
{% endmath %}

我们把$N^m_x$叫做未选择特征，$N^m_x=x/S^m_x$。

​		例如,对于一个模型解决情感分析任务,选择器只能学会选择token“amazing”如果这个token在输入文本中,预测器可能学习,当它接收输入$S_x^m={“amazing”}$,它预测最积极情绪得分。

**选择器-预测器模型目标在于对最小充分特征子集的解释。**例如，为了鼓励选择器做一项有意义的工作，而不是简单地返回所有特性(除非必要)，可以使用一个正则化器，根据所选特性的数量对模型进行比例惩罚，就像Lei等人(2016)的工作中所做的那样。因此，如公式2.4和2.5所定义的，这样的选择器预测模型的目标是一个最小的充分子集解释。例如，一个学习模仿图3.1中模型$m$的决策过程的选择器-预测器模型可能不会选择“nice”特性，如果“very good”是输入的子字符串。这是因为，在“very”和“good”之外选择“nice”并不会改变预测，但会增加所选特性的数量上的惩罚。 

**不能保证最小化**  虽然正则器可以激励选择特征数量的最小值，但不能保证一个选择器-预测器模型永远不会选择那些不是预测所严格需要的特征，因为这取决于模型在训练期间如何学习这样做。此外，如果对模型的查询小于$O(2^{|S^m_x|)}$，则检查所选特征集是否最小是一个有待解决的问题，因为所选特征的任何子集都可能是一个足够的最小子集。

​		从自解释模型的角度来看，不遵守最低限度条件有时可能是至关重要的。例如,如果学会了依靠伪相关模型,比如学习这个词“bottle”仅意味着积极的情绪,如果所选的特征包括真实情绪指数的话,那么所选择的特征所形成的解释会对用户隐藏模型所学习到的虚假关联。另一方面，正如3.1节中提到的，极小性意味着将相互抵消的特性视为不相关的，这可能并不总是需要的。因此，一个选择器-预测器模型可能并不总是满足最小条件，这一事实在实践中有时可能是有用的。

​		从验证的角度来看，如果某些被选择的特征与预测无关，那么因为解释器没有表现出这些特征比其他同样无关的特征更重要而惩罚解释器是错误的。

![](..\image\3.3.png)

图3.3:一个选择器-预测器模型的例子，它并不总是选择足够的特征子集。对于包含“very good”或“not good”的实例，“good”是预测的一个关键特征，但模型没有选择它。但是，对于任何不包含“very good”或“not good”的实例，模型不会呈现这种不需要的行为。

**不能保证充分性**  选择器-预测器模型的另一个问题是，选择的特征不能保证形成一个充分的子集，即模型可以依赖它没有选择的特征。更准确地说，$m$可以学习其选择器和预测器之间的内部紧急通信协议(Foerster et al.， 2016)，通过选择器和预测器在训练中达成一致的隐藏编码，使未被选择的特征对预测产生重要影响。图3.3给出了一个模型没有选择足够子集的例子。对于包含子字符串“very good”或“not good”的实例，例如$x^{(1)}$:“the movie is very good”。和$x^{(2)}$:“The movie is not good。”，“good”特征对预测有影响，但模型没有选择它。然而，对于不包含“very good”或“not good”子字符串的任何实例，相同的模型不会显示不希望看到的行为。在实践中，数据集的假相关性可能导致更误导的特征选择做选择预测模型。幸运的是，与检查最小条件不同，检查选择器-预测器模型$m$是否在实例$x$上选择了足够的子集要花费的时间少得多，我们有

{% math %}
$$
S^m_x\text{是一个充分子集}\leftrightarrow m(S^m_x)=m(x)
$$
{% endmath %}

因此，选择-预测模型无论是作为自解释模型还是作为验证解释者的实验台，都必须事先检查其充分性条件。



**在选择相互抵消特征时的不一致性。** 我们看到，选择器-预测器不能保证选择足够的特性的最小集合。因此，在每个实例中，这样的模型可能会选择某些相互抵消的特征组，而不会选择其他的。这种不一致性有优点也有缺点。一方面，这样的模型有机会选择形成真正取消的特征，而不是选择形成人工造成的取消的特征。最小充分子集解释器和Shapley解释器都不能做到这一点。另一方面，相反的情况也可能发生，也就是说，选择器-预测器模型可能选择形成人工造成的取消的特征，而不是选择形成真正的取消的特征。因此，用户需要意识到这种风险。这是一个开放的问题，是否可以以少于指数数量的查询到模型是否有非选择特征抵消彼此。



## 3.3 结论和开放式问题

​		在本章中，我揭示了使用基于特征的解释来解释模型的一些困难。首先，我说明了在实例上对模型的预测可以有不止一个基于真实特征的解释。我还展示了两种流行的基于特征的事后解释方法能够提供不同的事实解释，并且我发现了他们的优势和局限性。

​		其次，我研究了一种基于特征的自解释模型，称为选择器-预测器，并展示了它在提供自己的解释方面的优势和局限性。

​		本章的研究结果鼓励研究者直接陈述他们的解释器和验证框架的规格。他们也铺平了道路,进一步研究,如:(1)哪种类型的真实特征它们的解释或组合最适合用户在不同情况下的使用,和(2)在训练过程中是否可以对选择器-预测器模型进行正则化，以增强所选特征的充分性。