# 第六章 验证产生自然语言解释的神经网络

在本章中，基于(Camburu et al.， 2020)，我介绍了一个简单而有效的对抗性框架来验证神经模型是否能产生不一致的自然语言解释。



## 6.1 动机

为了解释黑箱神经模型产生的预测，越来越多的研究提出用自然语言解释生成模块来扩展这些模型，从而获得用人类语言解释自身的模型(Hendricks et al.， 2016;Park等人，2018年;Kim等，2018b;Jansen等，2018年;凌等，2017;Rajani等，2019)。

​		在本章中，我首先提请注意这样一个事实，即这些模型虽然很有吸引力，但却容易产生不一致的解释。如果两种解释对其要解释的实例和预测提出了相互矛盾的论点，那么这两种解释就被认为是不一致的。例如，考虑一个视觉问题回答(VQA)任务(Park et al.， 2018)和两个图像相同但问题不同的例子，比如“图像中有动物吗?”和“你能在图像中看到一只哈士奇吗?”首先，如果模型预测“是的”。并给出解释:“因为图片里有一只狗。”， 而对于第二种情况，同一个模型预测的结果是“否”。并生成解释:“因为图片里没有狗。”，那么这个模型就产生了两种不一致的解释。

​		不一致的解释至少揭示了以下不希望出现的行为之一:(i)至少有一个解释没有忠实地描述模型的决策过程，或者(ii)模型在至少一个实例中依赖于一个错误的决策过程。对于两种不一致的解释，需要进一步的调查来确定这两种行为中哪一种才是真正的行为(而且可能会因具体情况而有所不同)。事实上，两个前后矛盾的解释并不一定意味着至少有一个解释是不忠实的。在前面的例子中,如果图像包含一条狗,可能是模型识别狗当它处理图像与第一个问题,和模型不确定狗一起当它处理图像的第二个问题,因此这两种解释将忠实地反映模型的决策过程,即使他们是不一致的。同样，两个不一致的解释也不一定意味着模型依赖于一个错误的决策过程，因为这些解释可能不能忠实地描述模型的决策过程。

​		在研究如何确定这两种不期望的行为中哪一种对于一对不一致的解释是正确的之前，明智的做法是首先检查一个模型是否能够产生不一致的解释。为此，我介绍了InconsistVerif框架。对于一个给定的模型$m$和一个实例$x$,InconsistVerif旨在生成至少一个输入$\hat x$导致$m$来生成一个解释不一致的解释。因此,InconsistVerif属于的类别对抗的方法,例如,方法寻找输入导致模型产生不希望的答案(Biggio et al ., 2013;Szegedy等，2014)。

​		作为InconsistVerif的一部分，我解决了使用精确目标序列的对抗性攻击的问题，这是一个以前在顺序到顺序攻击中没有解决的场景，它对其他领域可能很有用，比如对话系统。最后，我将这个框架应用到前面章中提出的BiLSTM-Max-ExplPred-Att中，并表明这个模型可以产生大量不一致的解释。

## 6.2 相关工作

**验证产生自然语言解释的模型。**对产生自然语言解释的模型进行验证的工作非常少。Hendricks等人(2018)指出了在输入中没有任何证据存在的情况下，生成来自强类的属性的自然语言解释的风险。在这一章中，我将注意到另一个风险，即产生不一致的解释。

**产生敌对的例子。**生成对抗性实例是自然语言处理中一个活跃的研究领域(Zhang et al.， 2020;Wang等，2019)。例如，Jia和Liang (2017b)分析了抽取式问题回答模型对通过添加反向生成的分散文本获得的示例的鲁棒性。然而，大多数工作都是基于这样的要求，即对抗性输入应该是对原始输入的一个小干扰(Belinkov和Bisk, 2018;Hosseini等人，2017年;Cheng等，2018)，或者应该保持原始输入的语义(Iyyer等，2018)。我们的设置没有这个需求，任何一对导致模型产生不一致的解释的任务-现实输入就足够了。最重要的是，据我所知，序列对序列模型之前的对抗性攻击不会生成精确的目标序列，也就是说，给定一个序列，找到一个输入，使模型生成精确的给定序列。与这个目标最接近的是，Zhao等人(2018)提出了一个对抗框架，用于在机器翻译任务的目标序列中移除或添加token。类似地，Cheng等人(2018)要求在目标序列的任何位置都存在预定义的token。它们只测试最多三个需要的token，对于自动摘要任务，它们的成功率从一个标记的99%急剧下降到三个标记的37%。因此，他们的方法可能不能一般化到精确的目标序列。

​		最后，Minervini和Riedel(2018)试图找到在SNLI (Bowman et al.， 2015)上训练的模型违反一组逻辑约束的输入。理论上，这种情况可能导致寻找导致产生不一致解释的输入。但是，他们的方法需要枚举和计算输入中可能非常大的一组干扰，例如，删除子树或用同义词替换标记，因此计算成本很高。此外，他们的场景没有解决自动生成不需要的(不一致的)序列的问题。



## 6.3 框架

考虑一个模型$m$，它为每个实例$x$生成其对实例的预测的自然语言解释。我们将$m$对$x$的解释称为$e_m(x)$。对于实例$x$，我提出了一个框架，其目标是生成新的实例，模型为这些实例生成与$e_m(x)$不一致的解释。

​		InconsistVerif由以下高级步骤组成。对于实例$x$， (A)创建一个与$e_m(x)$不一致的解释列表，(B)在A中创建的列表中给出一个不一致的解释，找到一个导致$m$生成精确的不一致解释的输入。

**步骤。**我们所面临的设置有三个期望的属性，使它不同于在自然语言处理中通常研究的敌对设置:

- 在步骤(B)，模型必须生成一个确切的目标序列，因为目标是生成在步骤(A)中确定的与解释$e_m(x)$不一致的确切解释。
- 对抗性的输入不一定是一个解释或对原始输入的小干扰，因为目标是产生不一致的解释而不是不正确的预测——这些可能会作为副产品发生。
- 对抗性的投入必须对手头的任务是现实的。

据我所知，这项工作是解决这种设置的第一个工作，特别是由于生成准确的目标序列的挑战性要求，如6.2节所述。

**上下文相关的矛盾。**在某些任务中，实例由上下文(例如一幅图像或一段文字)和对上下文的一些评估(例如一个问题或一个假设)组成。由于解释可能涉及(有时隐含地)上下文，评估两种解释是否不一致也可能依赖于它。例如，在VQA中，两种解释的不一致“因为图像中有一只狗。”以及“因为图片中没有狗。”这取决于形象。然而，如果图像是相同的，这两种解释是不一致的，不管对图像提出了什么问题。

​		由于上述原因，给定一个实例$x$，我区分了实例中会在InconsistVerif中保持不变的部分((称为上下文部分，记为$x_c$)和InconsistVerif会将实例的部分改变，以获得不一致(称为可变部分，记为x_v),因此，$x=(x_c,x_v)$。在VQA中，$x_c$代表图片，而$x_v$ 代表问题。

**独立的不一致。**有些情况下，无论输入是什么，解释都是不一致的。例如，纯粹由背景知识形成的解释，如“女人是人”。和“女人不是人。”1 .总是前后矛盾(有时甚至是离谱的)，不管导致这些矛盾的原因是什么。在这些情况下,InconsistVerif可以把整个输入变量,也就是说,$x_c =∅$并且${\hat x_v}= x$。

**步骤** InconsistVerif包括以下步骤:

1. 通过训练一个RevExpl模型，将生成的解释和输入的上下文部分映射到输入的变量部分，即$RevExpl(x_c,e_m(x)) = x_v$。
2. 对于每一个解释$e=e_m(x)$:
   - 创建一个与$e$不一致的语句列表，称为$L_e$。
   - 对于每一个$\hat e \in L_e$ 以及上下文$x_c$。得到新的变量${\hat x}_v=REVEXPL(x_c,\hat e)$,这可能会导致产生不一致的解释。
   - 通过反向输入来获取$m$对于输入的反向解释$e_m(\hat x)$
   - 通过检查$e_m(\hat x) \in L_e$，来检查反向解释$e_m(\hat x)$是否确实和$e$不一致

为了执行步骤(2a)，对于任何任务，可以定义一组逻辑规则来将解释转换为不一致的对应项，例如否定或用特定任务的反义词替换任务本质标记。例如,在解释自动驾驶汽车(Kim et al ., 2018 b),可以将“绿灯”替换为“红灯”,或是“路是空的”与“道路拥挤”(特定于任务的反义词),得到不一致的和危险的解释如“汽车加速,因为是红灯。”。

​		获得不一致解释的另一种策略是交换来自相互排斥的标签的解释。例如，假设一个推荐系统预测电影X对于用户Y来说是一个糟糕的推荐，“因为X是一部恐怖电影”。，暗示用户Y不喜欢恐怖电影。如果同一个系统还预测movie Z是一个很好的推荐给同一个用户Y，“因为Z是一部恐怖电影。，那么我们就不一致了，因为后者暗示用户Y喜欢恐怖电影。

​		虽然这一步需要对手头的任务进行一定程度的具体调整，但可以论证的是，为了确保部署的系统是一致的，这是一个很小的代价。未来工作的一个有趣的方向是自动化这一步，例如，通过训练神经网络，在对手头任务的不一致解释的数据集进行群聚后，产生特定任务的不一致。

​		为了执行步骤(2d)， InconsistVerif当前检查一个反向解释和步骤(2a)中创建的任何不一致解释之间的确切字符串匹配。或者，人们可以训练一个模型来识别一对解释是否形成了不一致。

​		最后,尽管框架本身并不直接执行任务的对抗是现实的,他们是由一个模型,是在现实的数据引发的大多数对抗现实,我们会看到下面的实验。

## 6.4 实验

我对上一章介绍的模型BiLSTM-Max-ExplPred-Att进行不一致检验。我将$x_c$设置为前提(因为这代表了解决自然语言推理任务的给定上下文)，而$x_v$设置为假设。然而，由于SNLI的性质，决定大多基于常识知识，解释大部分时间独立于前提，如“狗是一种动物。”因此，也有可能逆转前提，而不仅仅是假设，这是留给未来的工作。

​		对于RevExpl模型，我使用与BiLSTM-Max-ExplPred-Att相同的神经结构和超参数。因此，RevExpl以一个前提解释对作为输入，生成一个假设。经过训练的RevExpl模型能够以32.78%的测试准确率重建出完全相同的假设(即字符串匹配)。

**生成** $L_e$。为了执行步骤(2a)，我使用了否定和交换解释。对于否定，我只需删除标记“not”和“n not”(如果它们存在的话)。如果这些标记在一个解释中出现不止一次，我就会产生多个不一致通过每次删除一个。我不尝试添加否定标记，因为这可能导致语法错误的句子。

​		为了交换e-SNLI上的解释，我们注意到这个数据集中的解释很大程度上遵循一组标签特定的模板。这是任务和SNLI数据集的自然结果，而不是e-SNLI集合的要求。例如，注释者经常使用“不能同时使用X和Y”。“解释一个矛盾”是中性的，“因为X不代表y”，或者“X暗示y”是含蓄的。由于任何两个标签是互斥的，从一个模板到另一个标签的模板转换解释应该自动创建一个不一致。例如，对于“人不能同时吃饭和睡觉”这一矛盾的解释。，其中一人将X与“吃”、Y与“睡”匹配，得出了“吃就睡觉”这一前后矛盾的解释。使用隐含模板“X意味着y”。因此,对于每一个标签,我创建了一个列表的最常用的模板e-SNLI我手动确定的解释,可以在附录c中找到一个运行的例子创建一致的解释通过交换在附录c。如果没有否定,没有模板匹配,实例被丢弃。在我们的实验中，只有2.6%的e-SNLI测试集因为这个原因被丢弃。这一过程可能导致语法或语义上不正确的不一致的解释。但是，我们将在下面看到，即使输入解释不正确，RevExpl也能很好地生成正确的和相关的逆向假设。这并不奇怪，因为RevExpl已经被训练成输出真实假设。

​		其余步骤如(2b) - (2d)中所述。更准确地说，对于$L_e$中每一个不一致的解释，RevExpl都返回一个反向假设，然后反馈给BiLSTM-Max-ExplPred-Att，得到一个反向解释。为了检查反向解释是否与最初的解释不一致，我检查了与步骤(2a)中不一致解释列表中的解释是否完全匹配的字符串。

**结果和讨论**。InconsistVerif标识了从包含9824个实例的e-SNLI测试集开始的总共1044对不一致的解释。然而，平均有1.93±1.77个不同的逆向假设，导致同一对不一致的解释。由于假设是不同的，每一个实例都是一个独立的有效的对抗性输入。然而，如果一个人严格地对不一致解释的不同对的数量感兴趣，那么，在消除重复之后，仍然有540对不同的不一致解释。

​		由于自然语言的生成总是由人类来评估的，所以我手工注释了100对随机的不同的对。我发现82%的逆向假设与前提一起构成了现实的实例。我还发现，绝大多数不现实的实例是由于假设中某个标记的重复造成的。例如，“一个小孩骑着头盔训练。是一个生成的逆向假设，非常接近一个完全有效的假设。

​		假设检测到的不一致中有82%是由现实的逆向假设引起的，那么InconsistVerif检测到约443对不同的不一致解释。虽然这意味着InconsistVerif在BiLSTM-Max-ExplPred-Att上的成功率只有~ 4.51%，但令人担忧的是，这个简单且未得到充分优化的对抗框架在一个以~ 570K示例为训练对象的模型上检测到大量的不一致性。在表6.1中，我们看到了检测到的不一致的例子。注意在给出相关前提的情况下，反向假设是如何现实的。

**手动扫描。**通过蛮力手动扫描，人们可以在多大程度上发现不一致，这是一个有趣的调查。我做了三个这样的实验，都没有成功。相反，当扫描Carmona等人(2018)提出的一般对抗性假设时，我注意到对不一致有很好的稳健性。

​		在第一个实验中，我手动分析了测试集中的前50个实例，没有发现任何不一致。然而，这些例子涉及不同的概念，因此减少了发现不一致的可能性。为了说明这一点，在第二次实验中，我通过简单的选择包含这些词的测试集中的解释，分别围绕woman、prisoner和snowboarding的概念构建了三组。我选择了这些概念，因为InconsistVerif检测到关于它们的不一致(见表6.1)。

​		对于女性，在测试集中有1150个例子，BiLSTM-MaxExplPred-Att生成了包含这个单词的解释。我随机查看了20个解释，没有发现任何不一致的地方。在单板滑雪中，测试集中有16个实例对bilstm - max - explelet产生了包含该词的解释，它们之间没有不一致。88对于prisoner，在测试集中只有一个实例BiLSTM-MaxExplPred-Att生成了包含该单词的解释，因此仅通过扫描测试集无法发现模型与该概念不一致。

​		我只看了测试集与不一致的公平比较，如果那只是应用在这个集。

​		即使人工扫描是成功的，它也不应该被认为是一个合适的基线，因为它不能带来与不一致扫描相同的好处。实际上，手动扫描需要相当多的人力来检查一大堆解释，以便发现其中任何两个是否不一致。即使只有50个解释，也需要不可忽视的时间。此外，将我们自己限制在原始数据集中的实例中，显然不如能够生成新实例有效。InconsistVerif框架解决了这些问题，并直接提供了对不一致的解释。尽管如此，这个实验对于解释BiLSTM-Max-ExplPred-Att并没有经常提供不一致的解释是有用的。

​		在手工扫描的第三个实验中，我从Carmona et al.(2018)引入的数据集中手工创建了一些假设。分别。对于模型是否会被它似乎表现出健壮性的概念(如toddler或swan)欺骗，进一步的研究需要在以后的工作中进行。这些假设已经被证明在标签层面上引起混淆。然而，BiLSTMMax-ExplPred-Att对所检查假设的不一致性显示了良好的稳健性水平。例如，对于中立的一对(前提:“一只鸟在水面之上。，假设:“天鹅浮在水面上。”)，BiLSTM-Max-ExplPred-Att产生了“不是所有的鸟都是天鹅”的解释。，而当“鸟”与“天鹅”互换时，即为一对(前提:“一只天鹅在水面之上。”，假设:“一只鸟在水面上。”)，BiLSTM-Max-ExplPred-Att产生了“天鹅是鸟”的解释。，很好地理解了实体“天鹅”和“鸟”之间的关系。同样的，在(前提:一个小孩通过一扇窗户看外面的世界。，假设:“一个蹒跚学步的孩子通过一扇窗户观察外面的世界。”)不会混淆模型，从而产生“不是每个孩子都是蹒跚学步的孩子”。和“一个蹒跚学步的孩子是一个小孩。”

## 6.5  结论和开放问题

在本章中，我提请注意一个事实，即产生自然语言解释的模型容易产生不一致的解释。这是一个普遍的问题，可以产生很大的实际影响。例如，如果自动驾驶汽车的解释模块容易说“汽车加速是因为没有人过十字路口”，用户可能不会接受自动驾驶汽车。还有“汽车加速是因为有人过十字路口。”

​		我介绍了一个识别这种不一致性的通用框架，并展示了在第5章中产生了对e-SNLI的正确解释的最高血统的模型BiLSTM-Max-ExplPred-Att，可以产生大量的不一致性。

​		有价值的对未来的工作方向包括:(1)为检测不一致,开发更先进的框架(2)调查是否不一致是由于不忠的解释或错误的决策过程的模型,和(3)发展中更健壮的模型,不产生矛盾。