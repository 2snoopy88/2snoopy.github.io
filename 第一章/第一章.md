# 1  简介

神经网络是一种计算系统，它被设计用来直接从图像或文本等原始数据学习从而执行任务，而不需要对特定任务的知识进行硬编码。由于它们的通用性和在各个领域的革命性成功，这些系统近年来越来越受欢迎。例如，在计算机视觉中，深度神经网络极大地提高了物体分类的技术水平，类似地，神经网络已经彻底改变了声学模型以及机器翻译，与传统的机器学习方法相比，在性能上有了显著的提升。不硬编码特定任务知识的学习是人工智能的一个突破。例如AlphaGo，仅仅通过观看大量人类棋局，然后反复与自己对弈，就学会了如何玩复杂的围棋游戏。2016年，AlphaGo以4比1击败了18次世界冠军李世石。此外，它还表明神经网络能够获得艺术能力和创造艺术。例如，神经网络可以识别艺术风格并将其转换成其他图像，并且他们也可以生成音乐。



## 1.1 解释深度神经网络的重要性 

​		研究表明，神经网络成功的一个关键因素是其深度的能力，例如，成功的神经网络都是由很多的非线性方法组成。直观地说，多层非线性函数允许网络在原始数据和预测之间的不同抽象层次上学习特征。然而，这是以可解释性为代价的，因为为大量非线性函数的复杂组成提供人类可理解的解释是一个困难的开放问题。因此，在安全关键的应用程序中，如健康诊断、信用额度或刑事司法，人们可能仍然倾向于使用不那么精确但可由人解释的模型，如线性回归和决策树。

​		关于神经网络的决策过程的怀疑是合理的，因为它已经表明，看起来非常精确的这种系统可以很容易地依赖数据集的虚假相关性(也称为统计偏差，或人工影响)来提供正确的答案。一个臭名昭著的例子是预测肺炎患者的糟糕结果，虽然在这方面，神经网络大大优于传统方法。然而，事实证明，训练集包含的模式是，有哮喘病史的患者死于肺炎的风险较低这样的模式。出现这种模式是因为哮喘患者得到了更快和更高的关注(因此，他们的死亡率更低)，因为事实上，他们的风险更高。显然，在实践中，使用依赖这种相关性的模型将是非常危险的。

​		黑箱系统中不信任的另一个来源来自这些系统可能发展的潜在主观偏见，如种族主义、性别歧视或其他类型的歧视和主观性。例如，Dressel and Farid 对商业风险评估软件COMPAS在惯犯预测中的公平性提出质疑。这种偏差可能是从我们训练和测试我们的模型的数据集中缺乏代表性或不相关的统计相关性中学习到的。例如，Bolukbasi等人(2016)表明，单词嵌入表现出女性/男性的性别刻板印象，而韦伯斯特等人(2018)表明，现有的语体和系统更青睐男性实体。

​		此外，大量的对抗性攻击已经显示出明显高度精确的神经网络的脆弱性。例如，图像处理神经网络只要通过对图像像素进行不可察觉的改变，就能将其任何预测变为任何其他可能的预测。神经网络中的对抗性攻击在其他领域也有显著的成功率，比如自然语言处理和语音识别。对抗性攻击揭示了深层神经网络的脆弱性，这使人们对这些方法的潜在的学习决策过程产生了怀疑。

​		因此，神经网络系统要获得广泛的公众信任，并确保这些系统确实是公平的，我们必须对这些模型的决策有人类可理解的解释。

​		首先，解释是必要的，它可以为最终用户(如病人和客户)提供正当的理由。提供这样的理由不仅是道德的，而且在一些国家，这甚至是法律所要求的，例如2016年GDPR中引入了“解释权”。

​		其次，解释对于这些系统的雇主，如医生和法官，是有用的，可以更好地了解一个系统的优势和局限性，并适当地信任和使用系统的预测。

​		第三，解释为知识发现提供了一个极好的来源。众所周知，神经网络在发现数据中的模式方面特别出色。因此，能够解释神经网络学习到的算法，可能会揭示人类难以从极大量的数据中挖掘的有价值的知识。例如，使用决策树从训练有素的神经网络中提取保护生物学知识。

​		最后，能够解释神经网络可以帮助这些方法的开发者/研究者诊断和进一步改进系统。例如，Ribeiro等人(2016)表明，在看到一些解释后，即使是非机器学习专家，也能够检测出哪些词应该从数据集中删除，以改进一个不值得信任的分类器。

​		鉴于上述解释的使用，目前对人工智能系统的预测的人类可理解的解释的需求越来越大，这也是我致力于这个主题的动机。



## 1.2 研究问题及大纲

​		鉴于自然语言解释数据集的稀缺，我首先在SNLI(一个现有的有影响力的自然语言推理数据集)上收集了一个大数据集，其中包含约570K个人类编写的自由形式的自然语言解释实例(Bowman et al.， 2015)。我把这个扩展数据集称为e-SNLI。例如，对于“一个女人在公园遛狗”这一事实的自然语言解释。与句子“A person is in the park.”相关。是“女人是人，在公园散步就意味着在公园里。”我将介绍在训练时从这些解释中学习并在测试时输出这些解释的神经模型。我还研究了使用来自自然语言解释的额外信号学习是否能在解决其他下游任务时提供好处。这些贡献将在第5章中介绍。

​		虽然神经模型生成的自然语言解释可以让用户放心，当他们为解决任务展示正确的论证时，它们可能不能忠实地反映模型的决策过程。因此，在本文中，我也调查了以下研究问题:**我们能否验证解释是否如实地描述了他们所要解释的深度神经网络的决策过程?**为了解决这类产生自然语言解释的神经模型的问题，可以考虑两种不同的方法。第一种方法是研究生成的自然语言解释是否符合每个模型。例如，如果一个模型产生了相互矛盾的解释，比如“图像中有一只狗。”以及“(同一幅)图像中没有狗。”，那么这个模型就存在一个缺陷。缺陷可能存在于决策过程中，也可能存在于生成的解释反映模型决策过程的准确性上(或者两者都存在)。我介绍了一个框架，它检查生成自然语言解释的模型是否会生成不一致的解释。作为这个框架的一部分，我将处理针对完整目标序列的对抗性攻击问题，这是一个以前在序列到序列攻击中没有处理过的场景，它对于其他类型的任务可能很有用。我在一个最先进的神经模型上实例化了这个框架，并表明这个模型能够产生大量的不一致性。这些贡献将在第6章中介绍。

​		第二种解释深层神经模型的方法包括开发事后解释方法，即旨在解释已经训练好的神经模型的外部方法。事后解释方法是广泛的，目前形成了技术的大多数提供解释的深度神经网络。虽然产生自己的自然语言解释的神经网络可能会被导致解释这类模型需要的相同类型的偏差所破坏，但外部的解释方法可能不太容易出现这些偏差。因此，检验模型产生的自然语言解释是否真实的一个有趣的方法就是考察外部解释方法所给出的解释与模型本身产生的自然语言解释之间的相关性。然而，外部解释方法也可能不能忠实地解释它们所要解释的模型的决策过程。因此，在本文中，我引入一个验证框架来验证外部解释方法的真实性。这些贡献将在第4章中介绍。

​		在开发上述验证框架的过程中，我发现了仅使用输入特征来解释模型的某些特性，即输入的单位，如文本的标记，图像的超像素。例如，我指出，有时对于一个模型的同一预测存在不止一种真实值解释，这与目前的文献似乎暗示的情况相反。我还说明了两种流行的后特殊解释方法针对的是不同的事实基础解释，并揭示了它们的一些优点和局限性。这些见解对于用户如何选择解释方法以最适合他们的需要，以及验证解释方法的忠实性，都有重要的影响。这些贡献将在第3章中介绍。

​		为了更好地理解这些贡献的范围，在深入研究这篇论文之前，我在第2章中提供了关于深度神经网络可解释性的背景知识。

​		在每一章的结尾，我提出关于本章的结论和一些开放性的问题。最后，在第七章，我提供了关于深度神经网络可解释性的一般结论和展望。