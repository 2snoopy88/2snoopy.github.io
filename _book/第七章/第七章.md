# 第七章 一般结论和观点

在这篇论文中，我研究了解释深层神经网络的两个主要方向:基于特征的事后解释方法和为预测生成自然语言解释的自解释神经模型。对于这两个方向，我都调查了验证这些解释是否忠实于它们所要解释的模型的决策过程的问题。此外，对于生成自然语言解释的自解释模型的课程，我调查了这些模型是否由于在训练时以自然语言解释的形式进行了额外的监督而表现出改进的行为。结果如下。

​		首先，我展示了，尽管有一个明显的隐含假设，即对一个实例的模型预测只有一种基于真实特征的解释，但通常会有更多这样的真实值解释。此外，我还展示了两类重要的事后解释方法，它们的目的是提供不同类型的基于事实的特征的解释，但没有明确说明，我还揭示了每种类型的某些优势和局限性。此外，对于某些情况，这些类型的解释都不足以为模型的决策过程提供一个明确的观点。我还说明了被期望为每个预测提供相关特性的选择器-预测器类型的神经模型在这样做时有一定的局限性，并且我提供了可以减轻这些局限性的解决方案。

​		为了在实践中正确使用这些方法，今后有必要对解释方法进行严格的规范。对解释如何提供模型决策过程的完整视图进行更多的研究也将是有价值的。此外，将选择器-预测器模型用于对决策过程提供可靠的见解也将是一个重要的研究方向。

​		其次，我介绍了一个框架来验证基于特征的事后解释方法所提供解释的真实性。该框架依赖于使用选择器-预测器模型作为目标模型，在这类模型的局限性得到缓解之后。这个框架是自动的，并在一个真实的场景中验证解释器，例如，在真实数据集上训练的非平凡的神经模型上。该框架也是通用的，因此，通过在选择器和预测器模块的其他任务和其他架构上实例化，可以生成潜在的大量现成的完整性测试。对三种常用解释方法的测试结果表明，该框架具有揭示这些方法所提供的不诚实解释的潜力。我还提出了进一步改进这个框架的方法。

​		作为今后的工作，可以进一步改进引进的核查框架，例如，使用我在第4.6节中介绍的准则。另外，其他类型的基于特性的自解释方法也可以用作验证解释者的测试平台。此外，请回忆一下，引入的框架是一个完整性检查，而不是一个评估框架，用于提供完全通用的解释器性能。也非常需要完整的评价框架。

​		第三，我研究了一类自我解释的神经模型，这些模型为他们的预测生成自然语言的解释。为了解决自然语言解释数据集的不足，我引入了一个新的大型数据集，数据集约为570K人类编写的自然语言解释，我称之为e-SNLI。此外，我还指出，神经模型依靠虚假的相关来提供正确的解释比提供正确的标签要困难得多。这从经验上支持了一种直觉，即能够产生正确的论证和正确的预测的模型比只提供正确预测的模型更可靠。此外，我还展示了在测试时生成自然语言解释的一系列模型在生成正确解释方面的性能相对较低。然而，我也展示了更好的架构可以带来改进。最后，我展示了在训练时出现的附加解释，这些解释引导模型学习更好的通用句子表示，以及更好地解决域外实例的能力，尽管这些改进对于被测试的模型来说微不足道。

​		未来的工作将进一步开发e-SNLI数据集，以推进研究的方向，训练和生成自然语言解释。本文中介绍的模型，虽然提供了令人鼓舞的结果，但还不能在现实世界中部署，需要做更多的工作来推进这个有希望的方向。新的自然语言解释数据集也将非常有益。

​		第四，为了验证生成的自然语言解释的真实性，我引入了一个对抗性框架来检查神经模型是否生成了不一致的自然语言解释。不一致的解释要么暴露了不诚实的解释，要么暴露了模型的错误决策过程，这两者都是不可取的。我将这个框架应用到在e-SNLI上产生最高比例正确解释的模型上，我证明了这个模型能够产生大量不一致的解释。此外，作为引入的框架的一部分，我解决了使用完整目标序列进行对抗性攻击的问题，这个场景以前在序列到序列攻击中没有研究过，它对于其他自然语言任务可能很有用。

​		验证由自解释模型产生的自然语言解释的真实性仍然是一个悬而未决的问题。本文引入的对抗性框架检验模型是否能产生不一致的自然语言解释。然而，这种不一致是由于不诚实的解释，还是由于模型决策过程中的缺陷，还需要进一步研究。此外，未来建立更稳健的模型，不会产生不一致的解释的工作也非常有价值。我的希望是，在未来，我们将会有强大和准确的神经模型，忠实地解释自己的人类语言。