# Chapter 2 深度神经网络解释的背景知识

​		本论文假设并没有描述机器学习，特别是神经网络的基本知识。对深度神经网络感兴趣的读者可以看看Goodfellow等人(2016)的《深度学习》(deep Learning)一书。特别地，我推荐第10章(“序列建模:递归和递归网络”)，因为递归神经网络在这篇论文中被广泛使用。本章的主要目的是提供解释方法的现状的必要背景，以方便阅读论文的其余部分，并把在此带来的领域的贡献的观点。

## 2.1 记号

本节描述了本论文中使用的符号。

**变量**。标量通常用非粗体字母表示，而向量用粗体表示。因此，我使用粗体小写罗马字母(如$x$)表示一维向量，假设它们是列向量。向量的一个分量通过将变量字母的非粗体版本下标为该分量的索引来表示。例如，向量$x$的第$i$个分量用$x_i$表示。

超参数通常用小写希腊字母表示。例如，一般来说，$\mu,\alpha$分别表示学习率和损失系数。

**数据**。数据。为了表示数据集，我使用了书法，大写罗马字母，例如$D$。为了枚举数据集中的实例(也称为样本或数据点)，我使用带括号的上标。例如，数据集$D=\{x^{(n)},y{(n)}\}_{n=1,N}$包含$n$个实例$x^{(n)}$及其关联标量或分类目标$y^{(n)}$。因此，为了表示数据集$D$中第$n$个实例的第$i$个组件，我使用$x_i^{(n)}$。在本文中特别重要的是由可变长度序列组成的数据点，如自然语言文本。对于输入序列$x$, $x_t$表示序列中的第t个时间步长。例如，在句子x中，$x_t$能够表示第t个标记或字符。对于离散输入，当神经网络在每个时间步长t处的实际输入是令牌$x_t$的嵌入向量(Mikolov et al.， 2013)时，我使用标量符号$x_t$表示时间步长t处的原始输入。

**方法**。功能。在本文中，函数的名称不包含其输入和输出的维数的指示。这些维度要么是陈述的，要么是从上下文推断的。与向量类似，函数输出的组件通过在参数(如果存在)前面的下标来引用。例如，函数$f$在x点的向量输出的第$i$个分量称为$f_i(x)$。一种常用的

一种常用的函数类型是用于训练神经网络的损失函数，我用大写字母L表示，它通常是损失项的总和。

最后，神经网络本身也是函数。一般模型通常被称为$m$。为了可读性，我使用缩写或首字母缩写作为神经网络(部分)的名称。例如，序列-序列神经网络的编码器和解码器可以分别称为enc(·)和dec(·)，多层感知器可以称为MLP(·)。

**大写$O$记号**。O(·)表示典型的大O符号。

**词汇**。在本文中，区分三种类型的算法是很重要的。首先，目标模型指的是我们想要解释的模型。第二，解释性方法(也称为explainer)是指旨在解释目标模型的方法。第三，验证框架是指验证解释方法是否忠实地解释目标模型的框架。



## 2.2 解释性方法的类型

​		最近，越来越多的不同作品旨在阐明深度神经网络(Ribeiro et al.， 2016;伦德伯格和李，2017年;Lapuschkin等人，2015年;Arras等人，2017年;Shrikumar等人，2017年;里贝罗等人，2018年;普拉姆布等人，2018年;Chen等，2018a;Kim等，2018b;Park等人，2018年;Lei et al.， 2016;Yoon等，2019)。这些方法在许多方面有本质上的不同。以下是解释方法的不同分组。但是，我不会介绍所有可能的解释方法。这是因为有大量的方法组,因为这一章的目的是提供背景投入角度看其余的论文,我不想讨论与这篇文章无关的一些方法。如果读者对解释方法的分类有兴趣，我可以参考Gilpin et al.(2018)和Arrieta et al.(2020)的著作。

### 2.2.1 事后解释和自解释

当前的解释方法之间最显著的区别将它们分为两类。

1. **事后解释方法**是一种独立的方法旨在解释已经训练和固定的目标模型，例如,**LIME**(里贝罗et al ., 2016)是一种因果解释的方法,解释了目标模型的预测通过学习可说明的模型,如线性回归,在附近的预测模型的概念(我将解释2.3节)的一个实例。

2. **自解释模型**是一种目标模型是否将解释生成模块合并到它们的体系结构中，以便为它们自己的预测提供解释的模型。在更高的层面上,自解释的模型有两个相互关联的模块:(i)*预测模块*,例如,模型的一部分,致力于预测手头的任务,和(ii)一个*解释模块*,即模型的一部分,致力于提供所做出预测的解释。例如，Lei等人(2016)引入了一种自解释神经网络，其中解释生成器选择输入特征的一个子集，然后独家传递给预测器，预测器仅根据所选特征提供最终答案。他们的模型也被正则化，这样选择就很短。因此，选择的特征旨在形成对预测的解释。

   自解释模型不一定需要对解释进行监督。例如，Lei et al. (2016)， Yoon et al. (2019)， Chang et al.(2019)所介绍的模型并不对解释进行监督，而只对最终的预测进行监督。另一方面，Park et al.(2018)、Kim et al. (2018b)、Hendricks et al.(2016)和Gilpin et al.(2018)引入的模型需要解释级的监督。

   

一般情况下，对于自解释模型，预测器和解释生成器是共同训练的，解释生成器的存在会影响预测器的训练。这不是事后解释方法的情况，不影响所有的由已经训练和固定的目标模型做出的预测。因此，当一个神经网络增加一个额外的解释生成器时，其任务性能明显低于只训练来执行该任务的神经网络时，人们可能更倾向于使用后一种模型，后面跟着一个事后解释方法。另一方面，也有可能用解释生成器增强神经网络，并联合训练它们，从而在手头的任务中获得更好的表现。这可能是由于模型体系结构中附加的指导，或者是对解释的额外监督(如果可以的话)。例如，在情绪分析任务上，Lei等人(2016)得出，在不对解释进行监督的情况下，添加中间解释生成器模块并不会影响性能。在常识性问题回答的任务上，Rajani等人(2019)通过对解释进行监督的自解释模型比仅训练完成该任务的神经网络获得了更好的表现。因此，这两种解释方法各有利弊。

在第三章中，我将介绍两种主要类型的事后解释方法之间的根本区别。在第4章中，我将介绍一个基于自解释目标模型的事后解释方法的验证框架。在第5章和第6章中，我将只关注自解释的模型。



### 2.2.2 黑盒和白盒

解释方法之间的另一个区别在于解释者需要的关于目标模型的知识。我们有以下两类。

1. **黑盒/模型无关的解释器** 是一种假设只能访问目标模型输入的解释器。例如**LIME** (Ribeiro et al.， 2016)、**Anchors** (Ribeiro et al.， 2018)、**KernalSHAP** (Lundberg and Lee, 2017)、**L2X** (Chen et al.， 2018a)和**LS-Tree** (Chen and Jordan, 2020)是模型不可知解释方法的几个例子。
2. **白盒/基于模型的解释器** 是一种假设可以访问目标模型体系结构的解释器，例如，LRP (Lapuschkin等，2015;Arras et al.， 2017)、DeepLIFT (Shrikumar et al.， 2017)、显著性地图(Simonyan et al.， 2014)、integrated gradients (Sundararajan et al.， 2017)、GradCAM (Selvaraju et al.， 2019)以及DeepSHAP和MaxSHAP (Lundberg and Lee, 2017)是模型依赖解释方法的几个例子。

这种划分主要适用于事后解释方法，因为在默认情况下，自解释模型在解释生成器和预测器之间表现出很强的联系。

​		黑箱解释器比白箱解释器具有更广泛的适用性，因为前者可以应用于无法访问目标模型内部结构的情况。黑箱解释器通常以现成的方式使用起来更快，因为不需要将依赖模型的技术适应于模型的特定架构或新的层类型。然而，这些优势是以可能不那么准确的解释为代价的，因为黑箱解释者可以推断输入和预测之间的相关性，而这并不一定反映目标模型的真实内部工作方式。

​		我将在第4章中介绍的验证框架可以应用于黑箱和白箱解释器。



### 2.2.3 基于实例和全局

划分解释方法的另一种方法是根据解释的范围。我们有以下两种类型的解释器。

1. **基于实例的解释器** 提供目标模型对于单个实例预测的解释。(Ribeiro等人，2016;伦德伯格和李，2017年;Chen等，2018a;普拉姆布等人，2018年;Lapuschkin等人，2015年;Arras等人，2017年;Simonyan等，2014;Shrikumar等人，2017年)。例如，LIME (Ribeiro et al.， 2016)通过在实例的邻域上训练线性回归来学习对任何实例的实例明智解释。
2. 全局整个目标模型的高层内部工作原理，例如Frosst和Hinton(2017)通过将神经网络提炼成软决策树来提供全局解释。

​        例如，对于终端用户需要解释其特定用例所采取的决策的用例，实例型解释器特别有用。全局解释器是特别有用的，例如，对于可能的偏差的快速模型诊断或知识发现。由于全局解释器的目的是解释整个目标模型的行为，通常是通过将目标模型提炼成可解释的模型，因此它们也隐含地提供了实例方面的解释。然而，对于一个可解释的模型来说，要准确地捕捉高度非线性模型学习到的所有不规则性，虽然不是不可能，单是很困难的。因此，来自全局解释者的实例解释可能并不总是准确的。目前大部分的文献工作集中在设计实例型解释器。

​		相反，实例解释可以是获得全局解释的起点。例如，Ribeiro et al.(2016)和Ribeiro et al.(2018)引入了子模块选取技术，从实例式的解释推导出模型内部工作的全局解释，而Ibrahim et al.(2019)使用聚类技术从实例式的解释出发，为子总体提供全局解释。

​		这种划分主要与事后解释方法有关。默认情况下，自解释模型是基于实例的解释器，因为内置的解释生成模块将应用于每个实例。然而，上面提到的从实例到全局解释的技术同样适用于自解释的模型。

​		本文仅针对实例型解释者。



### 2.2.4 解释的形式

解释方法也可以根据其所提供的解释的形式分为不同的组。下面我将简要描述几种常用的解释形式。我将更详细地讨论基于特征的和自然语言的解释，因为它们是本文的重点。

**基于特征的解释**。基于特征的解释是目前最广泛的解释形式，它包括评估一个实例的每个特征在该实例的模型预测中的重要性/贡献。常见的特征包括文本标记和图像超像素。

​		基于特征的解释有两种主要类型:重要性权重和特征子集。重要性权重解释为实例中的每个输入特征提供一个实数，表示该特征对模型对该实例的预测的贡献。子集解释为每个实例提供最重要特征的子集，用于对该实例进行模型预测。例如，一个模型可以预测“这部电影非常好”这句话。”有一个4星的积极情绪(5),可以形成一个子集的解释的特性{“非常”,“好”},而重要性权重的解释,例如,对于属性特性”好“有3的权重而对于属性“非常”有1的权重(重要性权重的总和与4的总预测匹配;这个属性称为特性加性，将在下一节中详细讨论)。我将在2.3节中介绍这两种基于特性的解释。

“这部电影非常好”：4星的积极情绪

| 重要性权重解释 | 特征子集解释 |
| :--------: | :------: |
| “好”：weight3 “非常”：weight4 | {“好”，“非常”} |

​		有大量的解释方法提供基于特征的解释。例如，LIME (Ribeiro et al.， 2016)、SHAP (Lundberg and Lee, 2017)、L2X (Chen et al.， 2018a)、LS-Tree (Chen and Jordan, 2020)、Anchors (Ribeiro et al.， 2018)、LRP (Lapuschkin et al.， 2015;Arras et al.， 2017)、DeepLIFT (Shrikumar et al.， 2017)、Integrated Gradients (Sundararajan et al.， 2017)和grado - cam (Selvaraju et al.， 2019)，而基于特征解释的自解释方法包括RCNN (Lei et al.， 2016)、INV ASE (Yoon et al.， 2019)、CAR (Chang et al.， 2019)，以及有影响力的注意力等级模型(Bahdanau et al.， 2015)等。只是众多基于特征的后特殊解释方法中的一小部分。

​		我在第3章和第4章研究基于特征的解释。

**自然语言解释**。自然语言解释由自然语言句子组成，这些句子提供类似人类的论点来支持预测。例如，对于“一个女人在公园遛狗”这一事实（继承自“一个人在公园”）的自然语言解释是“女人是人，在公园散步就意味着在公园里。”

​		为了训练解释方法提供自然语言解释，最近收集了新的人工书写自然语言解释数据集，以便在训练时监督解释，并在测试时评估生成的解释的正确性(Park et al.， 2018;Hendricks等人，2016年;Kim等，2018b;凌等，2017;Jansen等，2018年;Rajani等，2019)。例如Park et al.(2018)引入了ACT-X和VQA-X这两个数据集，分别包含(除了视觉解释外)用于视觉活动识别和视觉答疑任务的自然语言解释。此外，Park等人(2018)还引入了自解释模型PJ-X(指向和理由解释)，该模型被训练用于联合提供预测、基于特征的解释和自然语言解释，两种解释都支持预测。类似地，Kim等人(2018b)引入了BDD-X (Berkeley DeepDrive eXplanation)数据集，该数据集由支持自动驾驶汽车决策的自然语言解释组成。他们训练的模型组成的汽车控制器(预测)和一个解释发生器,无人驾驶汽车的决定是合理的用户与解释,如“汽车移动回到左边的车道,因为前面的校车停下来。”。另一项相关工作是Jansen等人(2018)的工作，他们为基础科学问题提供了自然语言解释图的数据集。然而，他们的语料库非常小，只有1680对问题和解释。类似地，Ling等人(2017)引入了用于解决数学问题的文本解释数据集。然而，这个数据集的焦点是狭窄的，而且很难转移到更一般的自然理解任务上。

​		虽然大多数自然语言解释方法都是自解释模型，但也有一些工作，如Hendricks等人(2018)的作品，旨在以事后的方式提高自然语言解释的质量。

​		在第5章中，我为解决自然语言推理这一有影响力的任务介绍了一个新的大型数据集(约570K自然语言解释)。此外，我开发了将自然语言解释纳入训练过程的模型，并在测试时生成这些解释。此外，在第6章中，我注意到	这样一个事实，即这样的模型可以产生不一致的自然语言解释，这暴露了模型中的缺陷。我介绍了一个对抗的框架来检测对不一致的解释，并展示了在第5章中训练的最好的模型可以产生大量不一致的解释。

**基于层次的解释 **基于概念的解释者旨在量化用户定义的高级概念(如卷曲或条带)对模型预测的重要性(Kim et al.， 2018a;(Akula等，2020)。

**基于实例的解释** 基于实例的解释提供实例的解释，从中人们可以获得对模型预测的洞察力。例如，对于给定的实例，Koh和Liang(2017)的目标是追踪训练集中哪些实例对当前实例的模型预测影响最大。Kanehira Harada(2019) 提供了一些例子，展示了模型区分当前实例和其他实例的能力，这些实例相对于当前实例只有很少的本质区别。

**代理解释 **代理解释器的目的是提供目标模型的可解释代理模型。例如，Alaa和van der Schaar(2019) 使用Meijer g函数对其代理解释器进行参数化。

**解释形式的混合** 一个解释器也可以提供一种以上的解释。例如Park et al.(2018)提出的PJ-X模型是一种既提供基于特征的解释又提供自然语言解释的自解释模型，对这两种解释进行联合训练可以提高这两种解释的能力。类似地，Kanehira和Harada(2019)引入了一种可以自我解释的模型，它可以生成自然语言和基于实例的解释。

## 2.3 基于特征的解释

如上所述，基于特征的解释有两种主要类型:重要性权重和特征子集。下面我将分别对它们进行描述。设$m$为模型，$x$为实例，$n$个特征的潜在数量可变，$x = (x1, x2，…)$(例如，$x_i$是句子$x$中的第$i$个标记)。基于特性的解释可以看作是x到输出空间的映射。为了重要性权重解释，将$x$每个特征映$x_i$射为一个实数。对于子集解释，$x$映射到$x$的子集集合，每个子集都是一个潜在的替代解释。

### 2.3.1 重要性权重

一个实例$x$中每个特征$x_i$的权重$w_i(m,x)$表示特征$x_i$对于预测$m(x)$的重要性(也称为贡献)(Ribeiro et al.， 2016;伦德伯格和李，2017年;Sundararajan等人，2017;Lapuschkin等人，2015年;Arras等人，2017年;Shrikumar等人，2017年;Simonyan等，2014;Chen and Jordan, 2020)。权重是有符号的实数，符号表示特征是将模型拉向预测(与预测符号相同的符号)还是与预测相反(与预测符号相反)。

#### 2.3.1.1 加性特征权重

**特征加性是一个在很多权重特征解释器中应用的性质，要求实例中出现的所有特征的重要性权重之和等于模型对该实例的预测减去模型的偏差。**对于分类任务，模型的预测被认为是预测类的概率，通常是每个可能类的预测概率中最高的概率。模型的偏差是模型对不带来任何信息的输入的预测，通常称为参考输入或基线输入。例如，全黑图像是计算机视觉的共同基线，而零矢量嵌入是自然语言处理的共同基线(Chen et al.， 2018a;Sundararajan等人，2017)。同样,在一个实例$x$中任何特征带来的信息可以通过阻塞(使用baseline特征来替代该特征,如黑色super-pixel或零向量嵌入)或遗漏(完全删除特征)(K´广告´ar et al ., 2017)消除，如果在技术上是可行的,例如,对于承认可变长度的输入序列的模型,如自然语言处理模型。如Sundararajan等人(2017)所述，遮挡和遗漏都可能导致输入分布不全，从而导致不可靠的重要性权重。

​		形式上，对于模型$m$和实例$x$，在特征可加性约束下，特征${x_i}$的赋值权重为$\{w_i(m,x)\}$，即

{% math %}
$$
\sum_{i=1}^{|x|}w_i(m,x)=m(x)-m(b) \tag{2.1}
$$

{% endmath %}

其中$b$为基线输入。对于回归模型，$m(x)$是模型对实例x的真实预测值，对于分类任务，$m(x)$是预测到的类的概率。

​	大量的解释方法依赖于特征加性(Ribeiro et al.， 2016;伦德伯格和李，2017年;Lapuschkin等人，2015年;Arras等人，2017年;Shrikumar等人，2017年;Simonyan等，2014;Sundararajan等人，2017)。例如，LIME (Ribeiro et al.， 2016)在每个实例$x$的邻域上学习一个局部线性回归模型，其中邻域由所有实例定义，这些实例通过将实例$x$中所有可能的特征组合设置为一个基线值。



**Shapley values** Lundberg和Lee(2017)的目标是统一特征加性解释方法，通过表明唯一一组特征加性重要性权重可以验证三个特性——*局部准确性、缺失性和一致性*——由合作博弈论中的Shapley值给出(Shapley,  1951)。

1. *局部精度*  (在Sundararajan等人(2017)中也称为完整性)要求等式2.1成立。请注意，并不是所有的特性方法都保证提供的重要性权重满足这个等式，即使这是它们的目标。例如，当LIME在实例$x$附近学习线性回归时，这个线性回归模型可能无法在实例$x$上产生它要解释的相同预测。

2. *缺失性*  要求实例$x$中不存在的特征对预测$m(x)$的重要性权重为零。例如,对于句子$x$的预测模型$m$, 只有出现在句子$x$中的token应该成为解释的一部分,因此可以给定一个非零的重要性权重,而其他词汇表中没有出现在句子中的token对于预测结果的重要性权重应该为0。

3. *一致性*  需要,如果两个模型$m$$和$${m}'$，实例$x$的一个特征$x_i$的边际贡献对于$m$来说比${m}'$高，那么$x_i$的重要性权重相应的也要高。形式上，对于一个特征子集$x_s\subseteq x$ ，我们有
   
      {% math %}
      $$
      m(x_s\cup\{x_i\})-m(x_s)\geq {m}'(x_s\cup\{x_i\})-{m}'(x_s)\tag {2.2}
      $$
      {% endmath %}
      
      那么就有$w_i(m,x)\geq w_i({m}',x)$

Lundberg和Lee(2017)表明Shapley值是唯一满足上述三个特性的解，并且可以用以下封闭形式进行计算

{% math %}
$$
w_i(m,x)=\sum_{x\subseteq x/{x_i}}\frac{|x_s|!(|x|-|x_s|-1)!}{|x|!}[m(x_s\cup\{x_i\})-m(x_s)] \tag{2.3}
$$
{% endmath %}

其中，$sum$枚举$x$中不包含特征$x_i$的所有特征子集$x_s$， |·|表示其参数的特征数量。对于那些特征的顺序很重要的任务，特征保持与它们在$x$中出现的顺序相同。

​		由于计算封闭形式的Shapley值需要在特征数量上花费指数时间，Lundberg和Lee(2017)提供了模型无关的(KernalSHAP)和模型依赖的(DeepSHAP和MaxSHAP)方法来逼近Shapley值。

#### 2.3.1.2 非特征加性权重

大多数重要性权重解释器是基于特征加性的。然而，不依赖于特性加性的重要性权重解释器也正在开发中。例如，LS-Tree (Chen和Jordan, 2020)利用语言数据的解析树来为每个特征分配权重，这样权重就可以用来检测和量化句子中标记之间的交互。与Shapley方法相似，LS-Tree也从合作博弈论中得到启发。然而，LS-Tree并没有使用Shapley值，而是受到Banzhaf值的启发(Banzhaf- iii, 1964)。



### 2.3.2 最小充分子集

另一个流行的解释模型在实例$x$上的预测的方法是提供一个最小特征子集$mss(m,x) \subseteq x$，通过这些最小充分子集，如果所有的其他特征都丢失，仅依靠这些特征就能使模型达到相同的预测，形式上，

{% math %}
$$
m(mss(m,s))=m(x)\\
\forall x_s \subset mss(m,x),有m(x_s)\neq m(x)
$$
{% endmath %}

​		要计算一个模型$m$对特征子集$x_s$的预测，可以对$x-x_s$中的特征进行遮挡或删除，如上所述。

​		最小性条件是保证不提供所有特征构成的平凡充分子集的必要条件。

​		目标模型并不仅仅总是依赖于特征的子集，相反可能也需要实例中的所有特征。如果需要全部输入来解释预测，那么这种解释将是无信息的。尽管如此，模型可能只依赖于每个实例输入特征的子集。例如，在计算机视觉中，并不是所有的像素都是模型分类一个对象或提供一个问题的答案所必需的。同样，在情绪分析中，某些子短语足以识别情绪。

​		一个实例可以有多个足够的最小子集。然后，每个最小充分子集都是对实例上的模型预测的一个独立的潜在解释。在这种情况下，要提供模型的完整视图，最好提供模型$m$和实例$x$的所有最小充分子集的集合，即$S(m,x) = \{mssk(m,x)\}_k$。在第3章中，我将揭露提供某些足够的最小子集作为解释的问题。

​		解释方法如L2X (Chen et al.， 2018a)、SIS (Carter et al.， 2019)、Anchors (Ribeiro et al.， 2018)和INV ASE (Yoon et al.， 2019)旨在提供此类解释。例如，L2X通过最大化模型对特征子集$m(x_s)$的预测与对整个实例m(x)的预测之间的相互信息来学习一个最小充分子集。然而，L2X假设一个最小充分子集的基数是已知的，并且在所有情况下都是相同的，这在实践中是一个主要的限制，可能会导致最小性和充分性条件被违反。相反，通过使用角色评论家方法(Peters和Schaal, 2008)， INVASE允许每个实例的最小充分子集的基数不同。L2X和INVASE都只提供了一个最小充分子集。另一方面，SIS提供了一组不重叠的最小充分子集。因此，SIS可能只标识整个集合$S(m,x)$的一个子集${S}'$。

​		在第3章中，我提供了更多关于基于基于shapley值解释和最小充分子集解释的优势和局限性的见解。

## 2.4	属性的解释

解释深层神经网络的目的是为了让人们理解这些模型的决策过程。因此，解释应该满足两个主要特性:解释应该是人类容易解释的，并且应该忠实地描述他们要解释的目标模型的决策过程。

**容易解释（理解）**  如果人类不能很容易地解释一个模型预测，那么对它的解释就没有意义。在极端情况下，由于神经网络是函数的组成，人们可以通过提供应用于实例的完整的函数链来解释任何预测，但这种解释对人类没有任何用处。

​		每一种解释都有自己的解释。例如,一个最小的足够的子集的解释是被解释为特性的一个子集,当去除剩下的特征，模型仍然能够做出相同的预测，且没有其他的最小充分特征子集能够得到相同的结论。

​		然而，并不是所有的解释方法都很容易被人类解释。例如，Shapley值形式的解释为每个特征给出了原始实例每个可能特征子集内的特征边际贡献的加权平均。可以论证的是，这样的数量是很难被人类解释的，人类可能只是求助于观察Shapley值的大小所赋予的特征的相对重要性，以及观察Shapley值符号所赋予的特征对模型的拉动方向。我将在第三章详细讨论这个话题。

**置信度**  置信度是指解释描述目标模型决策过程的准确性。解释的真实性不应与解释的属性混淆，解释为解决手头的任务提供了基础真理论证，这是独立于模型决策过程的。我将把后一个属性称为正确性(更多细节在第5章)。例如，如果目标模型内部只使用了解释提供的论证(以模型可理解的方式编码)来达到它的预测，那么自然语言解释就是忠实的。

​		解释直接影响目标模型中用户的感知和信任。因此，不忠实的解释可能是危险的，因为它会鼓励用户信任不可靠的和潜在的危险的模型，或者阻止用户信任完全可靠的模型。

​		如前所述，验证解释的真实性是本文的两个主要目标之一，将在第3、4、6章中具体说明。

