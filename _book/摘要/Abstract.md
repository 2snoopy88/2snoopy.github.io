# 摘要

​		深度神经网络在计算机视觉、自然语言处理和语音识别等不同领域取得了革命性的成功，因此越来越受欢迎。然而，这些模型的决策过程通常不能被用户解释。在各种领域，如医疗保健、金融或法律，了解人工智能系统所做决策背后的原因至关重要。

​		因此，最近研究了几个解释神经模型的方向。在这篇论文中，我研究了解释深层神经网络的两个主要方向。第一个方向由基于特征的事后解释方法组成，也就是说，这些方法的目标是解释一个已经训练和固定的模型(后特殊)，并根据输入特征提供解释，例如文本标记和图像超像素(基于特征)。第二个方向由产生自然语言解释的自解释神经模型组成，也就是说，模型有一个内置的模块，可以产生对模型预测的解释。在这些方面的贡献如下。

​		首先，我揭示了仅使用输入特性来解释即使是小模型也存在一定的困难。尽管有明显的隐含假设，即解释方法应该寻找一种特定的基于真实值特征的解释，但对于预测通常有不止一种这样的解释。我还展示了两类流行的解释方法，它们针对的是不同类型的事实基础解释，但没有明确地提及。此外，我还指出，有时，这两种解释都不足以提供一个在实例上的决策过程的完整观点。

​		其次，我介绍了一个框架，用于自动验证基于特征的事后解释方法对其目标解释的模型的决策过程的置信度。这个框架依赖于一种特定类型的模型的使用，这种模型用来提供对其决策过程的洞察。我分析了这种方法的潜在局限性，并介绍了减轻这些局限性的方法。引入的验证框架是通用的，可以在不同的任务和域上实例化，以提供现成的完整性测试，这些测试可用于测试基于特性的后特殊解释方法。我在情绪分析任务上实例化了这个框架，并提供了完备性测试，在此基础上我展示了三种流行的解释方法的性能。

​		第三，为了探索为预测生成自然语言解释的自解释神经模型的发展方向，我在有影响力的斯坦福自然语言(SNLI)数据集之上收集了一个巨大的数据集，数据集约为570K人类编写的自然语言解释。我把这个解释扩充数据集称为e-SNLI。我做了一系列的实验来研究神经模型在测试时产生正确的自然语言解释的能力，以及在训练时提供自然语言解释的好处。

​		第四，我指出，目前那些为自己的预测生成自然语言解释的自解释模型，可能会产生不一致的解释，比如“图像中有一只狗。”以及“（同一幅）图像片中没有狗”。不一致的解释要么表明解释没有忠实地描述模型的决策过程，要么表明模型学习了一个有缺陷的决策过程。我介绍一个简单而有效的对抗性框架，用于在生成不一致的自然语言解释时检查模型的完整性。此外，作为框架的一部分，我解决了使用精确目标序列的对抗性攻击的问题，这是一个以前在序列到序列攻击中没有解决的场景，它对于自然语言处理中的其他任务很有用。我将这个框架应用到e-SNLI上的一个最新的神经模型上，并表明这个模型会产生大量的不一致性。

​		这项工作为获得更稳健的神经模型以及对预测的可靠解释铺平了道路。



